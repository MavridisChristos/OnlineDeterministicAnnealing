{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Online Deterministic Annealing for Classification </center>\n",
    "\n",
    "<img src=\"tensors.jpeg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19e590e9190>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import HTML\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Dataset\n",
    "\n",
    "$X \\sim U[0,1]$\n",
    "\n",
    "$Y = 2X + 1 + 0.1 \\epsilon,\\ \\epsilon \\sim U[0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 1)\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n",
    "\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assume a parametrized model: \n",
    "$$ \\hat Y = f(X; \\theta),\\ \\theta \\in \\mathbb{R}^K $$\n",
    "and initialize $\\theta_i,\\ i=1,\\ldots,K$, randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.a + self.b * x\n",
    "    \n",
    "model = MyCustomModel().to(device) # model = nn.Sequential(nn.Linear(1, 1)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Determine a loss function: $J(\\hat Y, Y)$, e.g. \n",
    "\n",
    "$$J(\\hat Y, Y) = E[(\\hat Y - Y)^2] \\simeq \\frac 1 N \\sum_{i=1}^N (\\hat Y(\\omega_i) - Y(\\omega_i))^2,\\ \\omega_i \\in \\Omega$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss(reduction='mean') # nn.L1Loss, nn.CosineSimilarity, nn.CrossEntropyLoss, nn.NLLLoss, nn.KLDivLoss, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the gradient descent rule, e.g.\n",
    "\n",
    "$$ \\theta_i(j+1) = \\theta_i(j) - \\eta(j) \\frac \\partial {\\partial \\theta_i} J,\\ i=1,\\ldots, K,\\ j=1,\\ldots, n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr) # Adam, Adagrad, etc.\n",
    "\n",
    "def train_step(x, y):\n",
    "        model.train()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(y, yhat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For n epochs, do:\n",
    "\n",
    "- For every batch of the $N$ realizations of $X$ in your training data set compute $\\hat Y$, and $J(\\hat Y, Y)$.\n",
    "\n",
    "- Compute the gradients\n",
    "$$ \\frac \\partial {\\partial \\theta_i} J = \n",
    "\\frac {\\partial J} {\\partial f} \\frac {\\partial f} {\\partial @} \\ldots \\frac {\\partial @} {\\partial \\theta_i},\\ \n",
    "i=1,\\ldots, K$$\n",
    "and update the parameters\n",
    "\n",
    "- For every batch of the realizations of $X$ in your evaluation data set compute $\\hat Y$, and $J_v(\\hat Y, Y)$.\n",
    "\n",
    "- Save the optimal parameter values (with respect to $J_v$) in a file\n",
    "\n",
    "- Print Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([2.5952])), ('b', tensor([2.7504]))])\n",
      "[1] Training loss: 1.681\t Validation loss: 0.228\n",
      "[2] Training loss: 0.108\t Validation loss: 0.019\n",
      "[3] Training loss: 0.014\t Validation loss: 0.009\n",
      "[4] Training loss: 0.009\t Validation loss: 0.009\n",
      "[5] Training loss: 0.008\t Validation loss: 0.010\n",
      "[6] Training loss: 0.008\t Validation loss: 0.010\n",
      "[7] Training loss: 0.008\t Validation loss: 0.010\n",
      "[8] Training loss: 0.008\t Validation loss: 0.010\n",
      "[9] Training loss: 0.008\t Validation loss: 0.010\n",
      "[10] Training loss: 0.008\t Validation loss: 0.010\n",
      "OrderedDict([('a', tensor([1.0117])), ('b', tensor([1.9685]))])\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10 # 1000\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "loss_optimal = np.inf\n",
    "\n",
    "# Use previously saved state \n",
    "train_from_scratch = True\n",
    "if not train_from_scratch:\n",
    "    model.load_state_dict(torch.load(\"state_file.pth\"))\n",
    "\n",
    "print(model.state_dict())\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # Compute the loss in training data & update parameters for each batch\n",
    "    batch_losses = []\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        batch_losses.append(loss)\n",
    "    training_loss = np.mean(batch_losses)\n",
    "    training_losses.append(training_loss)\n",
    "\n",
    "    # Compute the loss in evaluation data\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            model.eval()\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(y_val, yhat).item()\n",
    "            val_losses.append(val_loss)\n",
    "        validation_loss = np.mean(val_losses)\n",
    "        validation_losses.append(validation_loss)\n",
    "        \n",
    "        # Save optimal parameters for re-training\n",
    "        if validation_loss < loss_optimal:\n",
    "            loss_optimal = validation_loss\n",
    "            torch.save(model.state_dict(), \"state_file.pth\")\n",
    "\n",
    "    # Print loss at each epoch\n",
    "    print(f\"[{epoch+1}] Training loss: {training_loss:.3f}\\t Validation loss: {validation_loss:.3f}\")\n",
    "\n",
    "print(model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
